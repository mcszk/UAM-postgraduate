{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetwarzanie tekstu na przykładzie strumienia danych z Twittera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Przetwarzanie tekstu (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Przykładowe przetwarzanie tekstu\n",
    "\n",
    "Poniższa komórka implementuje funkcję, która przyjmuje ciąg znaków (zmienną typu STRING) jako argument, modyfikuje ją i zwraca zmodyfikowaną wersję"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def sample_processing(text):\n",
    "    pass\n",
    "\n",
    "sample_text = \"This is a sample text. It will be modified by sample_processing function.\"\n",
    "processed_text = sample_processing(sample_text)\n",
    "print processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ekstrakcja hashtagów za pomocą wyrażeń regularnych\n",
    "\n",
    "Poniższa komórka implementuje funkcję, która tworzy listę hashtagów występujących w tweecie.\n",
    "\n",
    "https://docs.python.org/2/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_hashtags_list(tweet_text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Czyszczenie tweetów za pomocą wyrażeń regularnych\n",
    "Poniższa komórka implementuje funkcję, która za pomocą wyrażeń regularnych czyści tweety z linków, nazw użytkowników, itp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenizacja tweetów\n",
    "Poniższa komórka implementuje funkcję, która dzieli tekst tweeta na listę tokenów.\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stemming tokenów\n",
    "Poniższa komórka implementuje funkcję, która bierze jako argument listę tokenów i zwraca listę stemów.\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html<br>\n",
    "http://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Lematyzacja tweetów\n",
    "Poniższa komórka implementuje funkcję, która przyjmuje tekst tweeta jako argument i zwraca jego zlematyzowaną wersję.\n",
    "\n",
    "http://www.nltk.org/_modules/nltk/stem/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Lematyzacja przy użyciu części zdania (PoS tagging)\n",
    "Poniższa komórka implementuje funkcję, która przyjmuje tekst tweeta jako argument i zwraca listę par (token, część zdania).\n",
    "Następnie kolejna funkcja lematyzuje parę token w oparciu o rozpoznaną część zdania.\n",
    "\n",
    "http://www.nltk.org/api/nltk.tag.html<br>\n",
    "http://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Filtrowanie tokenów nie zawierających słów\n",
    "\n",
    "Poniższa komórka implementuje funkcję, która filtruje z listy tokenów tokeny nie zawierające żadnego znaku alfanumerycznego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Filtrowanie słów o małym znaczeniu \n",
    "\n",
    "Filtrowanie słów o małym znaczeniu odbywa się przy wykorzystaniu stop-list (ang. stopwords). Poniższa komórka wczytuje zapisaną na dysku listę z pliku tekstowego oraz implementuje funkcję, która w oparciu o tę listę filtruje słowa o małym znaczeniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'be', u'meeting', u'you', u'tomorrow', u'where', u'do', u'we', u'have', u'our', u'meeting', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "clear_regex = re.compile(ur'\\w')\n",
    "\n",
    "def filter_tokens(token_list):\n",
    "    filtered_tokens = []\n",
    "    for token in token_list:\n",
    "        if clear_regex.search(token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "lemma_tokens = [u'i', u'be', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "print filter_tokens(lemma_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Analiza sentymentalna tweetów\n",
    "Analiza sentymentalna polega na rozpoznaniu emocji występujących w tekście. Poniższa komórka implementuje metodę obliczającą pewną miarę liczbową sentymentu.\n",
    "\n",
    "https://github.com/cjhutto/vaderSentiment<br>\n",
    "http://www.nltk.org/_modules/nltk/sentiment/vader.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Rozpoznawanie encji (ang. Named entity recognition)\n",
    "Rozpoznawanie encji polega na indentyfikacji w zdaniach tokenów posiadających szczególne znaczenie. Typowe encje, które rozpoznaje się w tym procesie to: Osoba, Miejsce, Organizacja, Czas.\n",
    "Poniższa komórka implementuje metodę, która zwraca listy osób, miejsc i organizacji w danym tweecie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming danych z Twittera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Ustawienia kluczy i tokenów dla API Twittera\n",
    "\n",
    "W poniższej komórce ustawiane są zmienne niezbędne do uzyskania połączenia z API Twittera. Uzupełnij zmienne o swoje wartości kluczy i tokenów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_token = \"FILL IN WITH YOUR ACCESS TOKEN\"\n",
    "access_token_secret = \"FILL IN WITH YOUR ACCESS TOKEN SECRET\"\n",
    "consumer_key = \"FILL IN WITH YOUR CONSUMER KEY\"\n",
    "consumer_secret = \"FILL IN WITH YOUR CONSUMER KEY SECRET\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementacja klasy służącej do Streamingu danych z Twittera\n",
    "\n",
    "W poniższej komórce implementowana jest klasa służąca do pobierania streamu danych z Twittera. Klasa ta dziedziczy klasę StreamListener pochodzącą z biblioteki tweepy (biblioteki służącej do łączenia się z API Twittera za pomocą Pythona).\n",
    "Implementacja poniższej klasy modyfikuje domyślną metodę on_status(), która uruchamiana jest przy pojawieniu się każdego nowego statusu (tweeta) na Twitterze. \n",
    "\n",
    "Funkcja on_status() zapisuje każdego tweeta do bazy danych Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "\n",
    "class StreamProcessingListener(StreamListener):\n",
    "\n",
    "     def on_status(self, status):\n",
    "        description = status.user.description\n",
    "        loc = status.user.location\n",
    "        text = status.text\n",
    "        coords = status.coordinates\n",
    "        name = status.user.screen_name\n",
    "        user_created = status.user.created_at\n",
    "        followers = status.user.followers_count\n",
    "        id_str = status.id_str\n",
    "        created = status.created_at\n",
    "        retweets = status.retweet_count\n",
    "        bg_color = status.user.profile_background_color\n",
    "        \n",
    "        es.index(index=\"twitter\",\n",
    "             doc_type=\"tweet\",\n",
    "             body={\n",
    "                \"created_at\": created_at,\n",
    "                \"text\": text,\n",
    "                \"user_description\": user_description,\n",
    "                \"user_location\": user_location,\n",
    "                \"coords\": coords,\n",
    "                \"user_name\": user_name,\n",
    "                \"user_created\": user_created,\n",
    "                \"followers\": followers,\n",
    "                \"id_str\": id_str,\n",
    "                \"retweets\": retweets,\n",
    "                \"bg_color\": bg_color})\n",
    "        \n",
    "        print text\n",
    "        \n",
    "        return True\n",
    "    \n",
    "     def on_error(self, status):\n",
    "        print(status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Nawiązanie połączenia z API Twittera i uruchomienie stremingu\n",
    "\n",
    "W poniższej komórce nawiązywane jest połączenie z Twitterem za pomocą danych uwierzytelniających użytkownika a następnie uruchamiany jest 20 sekundowy streaming danych z przykładowym filtrem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n",
      "401\n",
      "401\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "listener = StreamProcessingListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "stream = Stream(auth, listener)\n",
    "\n",
    "#This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "stream.filter(track=['LePen'], async=True)\n",
    "time.sleep(20)\n",
    "stream.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
