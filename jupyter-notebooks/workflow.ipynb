{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetwarzanie tekstu na przyk≈Çadzie strumienia danych z Twittera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Przetwarzanie tekstu (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Przyk≈Çadowe przetwarzanie tekstu\n",
    "\n",
    "Poni≈ºsza kom√≥rka implementuje funkcjƒô, kt√≥ra przyjmuje ciƒÖg znak√≥w (zmiennƒÖ typu STRING) jako argument, modyfikuje jƒÖ i zwraca zmodyfikowanƒÖ wersjƒô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text. It will be modified by sample_processing function. #uam #bigdata #postgraduat√©\n"
     ]
    }
   ],
   "source": [
    "def sample_processing(text):\n",
    "    return text + ur\" #uam #bigdata #postgraduat√©\"\n",
    "\n",
    "sample_text = \"This is a sample text. It will be modified by sample_processing function.\"\n",
    "processed_text = sample_processing(sample_text)\n",
    "print processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ekstrakcja hashtag√≥w za pomocƒÖ wyra≈ºe≈Ñ regularnych\n",
    "\n",
    "Poni≈ºsza kom√≥rka implementuje funkcjƒô, kt√≥ra tworzy listƒô hashtag√≥w wystƒôpujƒÖcych w tweecie.\n",
    "\n",
    "https://docs.python.org/2/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#hashtag', '#anotherhashtag', '#YOLO']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_hashtags_list(tweet_text):\n",
    "    m = re.findall(ur'#\\w+', tweet_text, re.LOCALE)\n",
    "    return m\n",
    "\n",
    "print get_hashtags_list(\"Some sample tweet with some #hashtag, then some text and then #anotherhashtag again #YOLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenizacja tweet√≥w\n",
    "Poni≈ºsza kom√≥rka implementuje funkcjƒô, kt√≥ra dzieli tekst tweeta na listƒô token√≥w.\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'what', u'is', u'going', u'on', u'here', u'?', u'we', u'went', u'there', u'yesterday', u'.', u'where', u'have', u'you', u'gone', u'?', u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_tweets(tweet_text):\n",
    "    tokens = tokenizer.tokenize(tweet_text.lower())\n",
    "#     filtered_tokens = []\n",
    "#     for token in tokens:\n",
    "#         if clear_regex.search(token):\n",
    "#             filtered_tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "print tokenize_tweets(\"What is going on here? We went there yesterday. Where have you gone? I am meeting you tomorrow. Where do we have our meeting? Some sample tweet with some #hashtag, then some text and then #anotherhashtag again #YOLO. You aren't a bad Pyprogrammer\")\n",
    "# wpisz tutaj swojƒÖ funkcjƒô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stemming token√≥w\n",
    "Poni≈ºsza kom√≥rka implementuje funkcjƒô, kt√≥ra bierze jako argument listƒô token√≥w i zwraca listƒô stem√≥w.\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html<br>\n",
    "http://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'what', u'is', u'go', u'on', u'here', u'?', u'we', u'went', u'there', u'yesterday', u'.', u'where', u'have', u'you', u'gone', u'?', u'i', u'am', u'meet', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meet', u'?', u'some', u'sampl', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogramm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_tokens(token_list):\n",
    "    stem_list = []\n",
    "    for token in token_list:\n",
    "        stem_list.append(stemmer.stem(token))\n",
    "        \n",
    "    return stem_list\n",
    "\n",
    "my_token_list = [u'what', u'is', u'going', u'on', u'here', u'?', u'we', u'went', u'there', u'yesterday', u'.', u'where', u'have', u'you', u'gone', u'?', u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "\n",
    "print stem_tokens(my_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Lematyzacja tweet√≥w\n",
    "Poni≈ºsza kom√≥rka implementuje funkcjƒô, kt√≥ra przyjmuje tekst tweeta jako argument i zwraca jego zlematyzowanƒÖ wersjƒô.\n",
    "\n",
    "http://www.nltk.org/_modules/nltk/stem/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(token_list):\n",
    "    lemma_list = []\n",
    "    for token in token_list:\n",
    "        lemma_list.append(lemmatizer.lemmatize(token))\n",
    "        \n",
    "    return lemma_list\n",
    "\n",
    "my_token_list = [u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "print lemmatize_tokens(my_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Lematyzacja przy u≈ºyciu czƒô≈õci zdania (PoS tagging)\n",
    "Poni≈ºsza kom√≥rka implementuje funkcjƒô, kt√≥ra przyjmuje tekst tweeta jako argument i zwraca listƒô par (token, czƒô≈õƒá zdania).\n",
    "Nastƒôpnie kolejna funkcja lematyzuje parƒô token w oparciu o rozpoznanƒÖ czƒô≈õƒá zdania.\n",
    "\n",
    "http://www.nltk.org/api/nltk.tag.html<br>\n",
    "http://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meeting\n",
      "meet\n",
      "[u'i', u'be', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "print lemmatizer.lemmatize('meeting')\n",
    "print lemmatizer.lemmatize('meeting', 'v')\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "pos_tagger = FeaturesetTaggerI()\n",
    "\n",
    "def lemmatize_tokens_pos(token_list):\n",
    "    token_pos_list = pos_tag(token_list)\n",
    "    lemma_list = []\n",
    "    for token_pos in token_pos_list:\n",
    "        word = token_pos[0]\n",
    "        treebank_pos = token_pos[1]\n",
    "        wordnet_pos = get_wordnet_pos(treebank_pos)\n",
    "        lemma_list.append(lemmatizer.lemmatize(word, wordnet_pos))\n",
    "        \n",
    "    return lemma_list\n",
    "\n",
    "my_token_list = [u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "\n",
    "print lemmatize_tokens_pos(my_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Filtrowanie token√≥w nie zawierajƒÖcych s≈Ç√≥w\n",
    "\n",
    "Poni≈ºsza kom√≥rka implementuje funkcjƒô, kt√≥ra filtruje z listy token√≥w tokeny nie zawierajƒÖce ≈ºadnego znaku alfanumerycznego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'be', u'meeting', u'you', u'tomorrow', u'where', u'do', u'we', u'have', u'our', u'meeting', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "clear_regex = re.compile(ur'\\w')\n",
    "\n",
    "def filter_punctuation(token_list):\n",
    "    filtered_tokens = []\n",
    "    for token in token_list:\n",
    "        if clear_regex.search(token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "lemma_tokens = [u'i', u'be', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "print filter_punctuation(lemma_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Filtrowanie s≈Ç√≥w o ma≈Çym znaczeniu \n",
    "\n",
    "Filtrowanie s≈Ç√≥w o ma≈Çym znaczeniu odbywa siƒô przy wykorzystaniu stop-list (ang. stopwords). Poni≈ºsza kom√≥rka wczytuje zapisanƒÖ na dysku listƒô z pliku tekstowego oraz implementuje funkcjƒô, kt√≥ra w oparciu o tƒô listƒô filtruje s≈Çowa o ma≈Çym znaczeniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'meeting', u'tomorrow', u'meeting', u'sample', u'tweet', u'#hashtag', u'text', u'#anotherhashtag', u'again', u'#yolo', u\"aren't\", u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "with open ('common-english-words.txt', 'r') as stopwords_file:\n",
    "    raw_stopwords = stopwords_file.readlines()\n",
    "    \n",
    "stopwords = raw_stopwords[0].split(',')\n",
    "\n",
    "def filter_stopwords(token_list, stopwords_list):\n",
    "    filtered_tokens = []\n",
    "    for token in token_list:\n",
    "        if token not in stopwords_list:\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "lemma_tokens = [u'i', u'be', u'meeting', u'you', u'tomorrow', u'where', u'do', u'we', u'have', u'our', u'meeting', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "print filter_stopwords(lemma_tokens, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Analiza sentymentalna tweet√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5252\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_tweet_sentiment(tweet_sentence):\n",
    "    scores = sentiment_analyzer.polarity_scores(tweet_sentence)\n",
    "    return scores['compound']\n",
    "\n",
    "print calculate_tweet_sentiment(\"What is going on here? We went there yesterday. Where have you gone? I am meeting you tomorrow. Where do we have our meeting? Some sample tweet with some #hashtag, then some text and then #anotherhashtag again #YOLO. You aren't a bad Pyprogrammer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Rozpoznawanie encji (ang. Named entity recognition)\n",
    "Rozpoznawanie encji polega na indentyfikacji w zdaniach token√≥w posiadajƒÖcych szczeg√≥lne znaczenie. Typowe encje, kt√≥re rozpoznaje siƒô w tym procesie to: Osoba, Miejsce, Organizacja, Czas.\n",
    "Poni≈ºsza kom√≥rka implementuje metodƒô, kt√≥ra zwraca listy os√≥b, miejsc i organizacji w danym tweecie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wpisz tutaj swojƒÖ funkcjƒô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming danych z Twittera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Ustawienia kluczy i token√≥w dla API Twittera\n",
    "\n",
    "W poni≈ºszej kom√≥rce ustawiane sƒÖ zmienne niezbƒôdne do uzyskania po≈ÇƒÖczenia z API Twittera. Uzupe≈Çnij zmienne o swoje warto≈õci kluczy i token√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_token = \"2362404584-MJuLY5ISJq3CFxyDTVhuhI6rRjygCDxd9QYEzWg\"\n",
    "access_token_secret = \"pGD3PyMuz5M6YxzCkAryaytkPD0Eb2lF8q2aI9mNgg07o\"\n",
    "consumer_key = \"uOO1duKiGl0jaRBA9dRvewGXd\"\n",
    "consumer_secret = \"Odsld4Q5fAB9mk9VSJQUPYGDWcepOOUEZZk08Ya9CIR54szd4k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementacja klasy s≈Çu≈ºƒÖcej do Streamingu danych z Twittera\n",
    "\n",
    "W poni≈ºszej kom√≥rce implementowana jest klasa s≈Çu≈ºƒÖca do pobierania streamu danych z Twittera. Klasa ta dziedziczy klasƒô StreamListener pochodzƒÖcƒÖ z biblioteki tweepy (biblioteki s≈Çu≈ºƒÖcej do ≈ÇƒÖczenia siƒô z API Twittera za pomocƒÖ Pythona).\n",
    "Implementacja poni≈ºszej klasy modyfikuje domy≈õlnƒÖ metodƒô on_status(), kt√≥ra uruchamiana jest przy pojawieniu siƒô ka≈ºdego nowego statusu (tweeta) na Twitterze. \n",
    "\n",
    "Funkcja on_status() zapisuje ka≈ºdego tweeta do bazy danych Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200/\")\n",
    "\n",
    "\n",
    "class StreamProcessingListener(StreamListener):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        created_at = status.created_at\n",
    "        text = status.text\n",
    "        user_description = status.user.description\n",
    "        user_location = status.user.location\n",
    "        coords = status.coordinates\n",
    "        user_name = status.user.screen_name\n",
    "        user_created = status.user.created_at\n",
    "        followers = status.user.followers_count\n",
    "        id_str = status.id_str\n",
    "        retweets = status.retweet_count\n",
    "        bg_color = status.user.profile_background_color\n",
    "        \n",
    "        processed_text = sample_processing(text)\n",
    "        hashtags_list = get_hashtags_list(processed_text)\n",
    "        tokens = tokenize_tweets(processed_text)\n",
    "        stems = stem_tokens(tokens)\n",
    "        lemmas = lemmatize_tokens_pos(tokens)\n",
    "        filtered_tokens = filter_punctuation(tokens)\n",
    "        filtered_tokens = filter_stopwords(filtered_tokens, stopwords)\n",
    "        sentiment = calculate_tweet_sentiment(processed_text)        \n",
    "        \n",
    "        es.index(index=\"twitter\",\n",
    "             doc_type=\"tweet\",\n",
    "             body={\n",
    "                \"created_at\": created_at,\n",
    "                \"text\": text,\n",
    "                \"user_description\": user_description,\n",
    "                \"user_location\": user_location,\n",
    "                \"coords\": coords,\n",
    "                \"user_name\": user_name,\n",
    "                \"user_created\": user_created,\n",
    "                \"followers\": followers,\n",
    "                \"id_str\": id_str,\n",
    "                \"retweets\": retweets,\n",
    "                \"bg_color\": bg_color,\n",
    "                \"processed_text\": processed_text,\n",
    "                \"hashtags_list\": hashtags_list,\n",
    "#                 \"stems\": stems,\n",
    "#                 \"lemmas\": lemmas,\n",
    "                \"tokens\": filtered_tokens,\n",
    "                \"sentiment\": sentiment})\n",
    "\n",
    "        \n",
    "        print \"text\" + text\n",
    "        print sentiment\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 NawiƒÖzanie po≈ÇƒÖczenia z API Twittera i uruchomienie stremingu\n",
    "\n",
    "W poni≈ºszej kom√≥rce nawiƒÖzywane jest po≈ÇƒÖczenie z Twitterem za pomocƒÖ danych uwierzytelniajƒÖcych u≈ºytkownika a nastƒôpnie uruchamiany jest 20 sekundowy streaming danych z przyk≈Çadowym filtrem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textRT @EinzelfallXy: @HartesGeld Screenshot von @HartesGeld \n",
      "zu #LePen #Macronleaks https://t.co/plLf2a8P74\n",
      "0.0\n",
      "textRT @Purple_POV: Make France Great Again. üá´üá∑ @MLP_officiel #Bayrougate #lepen #presidentielle2017¬† #jevotepour¬† #ChoisirLaFrance‚Ä¶ \n",
      "0.6249\n",
      "textRT @AndreaChalupa: Please, please, please may the French media be more sophisticated and intelligent than the American media. Please. #EMLe‚Ä¶\n",
      "0.9366\n",
      "text@dodt2003 @marioadragna69 @GeorgiaDirtRoad @Root4Change @gearmeister @InTheYear1611 @MsRotti @tamaraleighllc‚Ä¶ https://t.co/LE8D2rYC7u\n",
      "0.0\n",
      "textRT @GayRepublicSwag: Who would you rather see as the next President of #France retweet for a large sample size. #LePen #Macron #MacronGate\n",
      "0.0\n",
      "textRT @vicktop55: French voters are most polarized in Europe ahead of key presidential runoff ‚Äì poll https://t.co/g2enxxnvTs\n",
      "0.0\n",
      "textRT @ProudWednesdayy: REWTEET if you stand with Marine LePen.  THIS is what a STRONG WOMAN looks like #saturdaymorning #jevotepour‚Ä¶ \n",
      "0.7603\n",
      "textRT @CodeAud: #ISIS calls for French election day attacks &amp; urges slaughter of #LePen &amp; #Macron https://t.co/UsZXP7ZVDu https://t.co/FKBcCCb‚Ä¶\n",
      "-0.4404\n",
      "text@TerrySechelski I agree but I still have hope that LePen can turn it around ....KeK wills it!\n",
      "0.7088\n",
      "textRT @sto93_NotS: @mom_vet @Kobayautiste @Shamira_1925 @nia4_trump @Dutch_Deplorabl @SiriusMatdj @VictorOfKadesh @KansasCity1980‚Ä¶ \n",
      "0.0\n",
      "textRT @MattAsherS: We are tweeting in support of #MacronPresident from 3-4ET b/c #altright &amp; #Russia üíó #LePen.\n",
      "\n",
      "Join us &amp; use‚û°Ô∏è‚Ä¶ \n",
      "0.5994\n",
      "textRT @Pamela_Moore13: #MacronLeaks\n",
      "\n",
      "Dems: Trump shouldn't support #LePen. It's interfering w/ the French election\n",
      "Obama: \"I support‚Ä¶ \n",
      "-0.3089\n",
      "textRT @wmmII88: #MSM in full panic as #MacronLeaks spreads to primetime TV. Keep up the pressure. We are the media now. #LePen https://t.co/82‚Ä¶\n",
      "-0.6705\n",
      "textRT @Roses_4_Thorns: #MSM in full panic as #MacronLeaks spreads to primetime TV. Keep up the pressure. We are the media now. #LePen https://‚Ä¶\n",
      "-0.6705\n",
      "textRT @MattAsherS: The American resistance supports #macronpr√©sident in #ElectionsPresidentielles2017üá´üá∑ because voting for #LePen is voting fo‚Ä¶\n",
      "0.3612\n",
      "textRT @IMPL0RABLE: #TheResistance #MacronLeaks #Macron #LePen #AVote\n",
      "\n",
      "Interesting thread: https://t.co/RD4cYP60bB\n",
      "0.4019\n",
      "textRT @sto93_NotS: @mom_vet @Kobayautiste @Shamira_1925 @nia4_trump @Dutch_Deplorabl @SiriusMatdj @VictorOfKadesh @KansasCity1980‚Ä¶ \n",
      "0.0\n",
      "textRT @GrrrGraphics: #MacronLeaks #MacronGate #frenchelection #BenGarrison #cartoon Welcome Mat for #Terror #JeVote #LePen #JeVoteMarine‚Ä¶ \n",
      "0.4588\n",
      "textvoters hurling eggs and flour??? https://t.co/whBxWsX6hF\n",
      "0.0\n",
      "text@klaasvaak1533 Wilders lost and so will LePen! https://t.co/L5f2q0IsAr\n",
      "-0.3802\n",
      "textRT @LindaSuhler: French Presidential Voting Underway as Overseas Electorate Head to Polls\n",
      "#VoteMarine #LePen #Frexit‚Ä¶ \n",
      "0.0\n",
      "textRT @Mags661: üí•üö® THE RUSSIANS!!!!!üö®üî•\n",
      "#MacronLeaks\n",
      "\n",
      "#LePen üá´üá∑ https://t.co/JetrJkA7Bd\n",
      "0.0\n",
      "textRT @Purple_POV: Make France Great Again. üá´üá∑ @MLP_officiel #Bayrougate #lepen #presidentielle2017¬† #jevotepour¬† #ChoisirLaFrance‚Ä¶ \n",
      "0.6249\n",
      "textRT @AFP: Paradise lost: idyllic French villages turn to far right - @ADAMPLOW reports https://t.co/ZjBP3tztDY‚Ä¶ \n",
      "0.4404\n",
      "textRT @womensmarchlon: France. On Sunday show the world you stand for inclusion and equality. Say no to LePen #VoteAgainstHate #chooselove htt‚Ä¶\n",
      "-0.296\n",
      "textRT @starbuck2017: We're supporting #JeSoutiensMacron from 3-4ET Sat #Altright &amp; #Russia üíó#LePen\n",
      "Join us &amp; use #Presidentielle2017¬† --‚Ä¶ \n",
      "0.6249\n",
      "textRT @ChooseToBFree: üá´üá∑MAKE FRANCE GREAT AGAINüá´üá∑\n",
      "#MarineLePen‚úÖ\n",
      "#France\n",
      "#2017LeD√©bat¬† \n",
      "#Debat2017¬†\n",
      "#JeVote¬†\n",
      "#LePen\n",
      "#Presidentielle2017 https:/‚Ä¶\n",
      "0.7034\n",
      "textRT @Mags661: üí•üö® THE RUSSIANS!!!!!üö®üî•\n",
      "#MacronLeaks\n",
      "\n",
      "#LePen üá´üá∑ https://t.co/JetrJkA7Bd\n",
      "0.0\n",
      "textRT @North_Going_Zax: Vote smart vote to stop the British LePen @theresa_may &amp; her far right coalition of chaos https://t.co/hYMGyyb3Tt\n",
      "-0.4939\n",
      "textRT @GrrrGraphics: #MacronLeaks #MacronGate #frenchelection #BenGarrison #cartoon Welcome Mat for #Terror #JeVote #LePen #JeVoteMarine‚Ä¶ \n",
      "0.4588\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "listener = StreamProcessingListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "stream = Stream(auth, listener)\n",
    "\n",
    "#This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "stream.filter(track=['LePen'], languages=['en'], async=True)\n",
    "time.sleep(60)\n",
    "stream.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
