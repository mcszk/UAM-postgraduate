{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przetwarzanie tekstu na przykładzie strumienia danych z Twittera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Przetwarzanie tekstu (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Przykładowe przetwarzanie tekstu\n",
    "\n",
    "Poniższa komórka implementuje funkcję, która przyjmuje ciąg znaków (zmienną typu STRING) jako argument, modyfikuje ją i zwraca zmodyfikowaną wersję"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text. It will be modified by sample_processing function. #uam #bigdata #postgraduaté\n"
     ]
    }
   ],
   "source": [
    "def sample_processing(text):\n",
    "    return text + ur\" #uam #bigdata #postgraduaté\"\n",
    "\n",
    "sample_text = \"This is a sample text. It will be modified by sample_processing function.\"\n",
    "processed_text = sample_processing(sample_text)\n",
    "print processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ekstrakcja hashtagów za pomocą wyrażeń regularnych\n",
    "\n",
    "Poniższa komórka implementuje funkcję, która tworzy listę hashtagów występujących w tweecie.\n",
    "\n",
    "https://docs.python.org/2/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#hashtag', '#anotherhashtag', '#YOLO']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_hashtags_list(tweet_text):\n",
    "    m = re.findall(ur'#\\w+', tweet_text, re.LOCALE)\n",
    "    return m\n",
    "\n",
    "print get_hashtags_list(\"Some sample tweet with some #hashtag, then some text and then #anotherhashtag again #YOLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenizacja tweetów\n",
    "Poniższa komórka implementuje funkcję, która dzieli tekst tweeta na listę tokenów.\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'what', u'is', u'going', u'on', u'here', u'?', u'we', u'went', u'there', u'yesterday', u'.', u'where', u'have', u'you', u'gone', u'?', u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_tweets(tweet_text):\n",
    "    tokens = tokenizer.tokenize(tweet_text.lower())\n",
    "#     filtered_tokens = []\n",
    "#     for token in tokens:\n",
    "#         if clear_regex.search(token):\n",
    "#             filtered_tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "print tokenize_tweets(\"What is going on here? We went there yesterday. Where have you gone? I am meeting you tomorrow. Where do we have our meeting? Some sample tweet with some #hashtag, then some text and then #anotherhashtag again #YOLO. You aren't a bad Pyprogrammer\")\n",
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stemming tokenów\n",
    "Poniższa komórka implementuje funkcję, która bierze jako argument listę tokenów i zwraca listę stemów.\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html<br>\n",
    "http://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'what', u'is', u'go', u'on', u'here', u'?', u'we', u'went', u'there', u'yesterday', u'.', u'where', u'have', u'you', u'gone', u'?', u'i', u'am', u'meet', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meet', u'?', u'some', u'sampl', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogramm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_tokens(token_list):\n",
    "    stem_list = []\n",
    "    for token in token_list:\n",
    "        stem_list.append(stemmer.stem(token))\n",
    "        \n",
    "    return stem_list\n",
    "\n",
    "my_token_list = [u'what', u'is', u'going', u'on', u'here', u'?', u'we', u'went', u'there', u'yesterday', u'.', u'where', u'have', u'you', u'gone', u'?', u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "\n",
    "print stem_tokens(my_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Lematyzacja tweetów\n",
    "Poniższa komórka implementuje funkcję, która przyjmuje tekst tweeta jako argument i zwraca jego zlematyzowaną wersję.\n",
    "\n",
    "http://www.nltk.org/_modules/nltk/stem/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(token_list):\n",
    "    lemma_list = []\n",
    "    for token in token_list:\n",
    "        lemma_list.append(lemmatizer.lemmatize(token))\n",
    "        \n",
    "    return lemma_list\n",
    "\n",
    "my_token_list = [u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "print lemmatize_tokens(my_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Lematyzacja przy użyciu części zdania (PoS tagging)\n",
    "Poniższa komórka implementuje funkcję, która przyjmuje tekst tweeta jako argument i zwraca listę par (token, część zdania).\n",
    "Następnie kolejna funkcja lematyzuje parę token w oparciu o rozpoznaną część zdania.\n",
    "\n",
    "http://www.nltk.org/api/nltk.tag.html<br>\n",
    "http://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meeting\n",
      "meet\n",
      "[u'i', u'be', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "print lemmatizer.lemmatize('meeting')\n",
    "print lemmatizer.lemmatize('meeting', 'v')\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "pos_tagger = FeaturesetTaggerI()\n",
    "\n",
    "def lemmatize_tokens_pos(token_list):\n",
    "    token_pos_list = pos_tag(token_list)\n",
    "    lemma_list = []\n",
    "    for token_pos in token_pos_list:\n",
    "        word = token_pos[0]\n",
    "        treebank_pos = token_pos[1]\n",
    "        wordnet_pos = get_wordnet_pos(treebank_pos)\n",
    "        lemma_list.append(lemmatizer.lemmatize(word, wordnet_pos))\n",
    "        \n",
    "    return lemma_list\n",
    "\n",
    "my_token_list = [u'i', u'am', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "\n",
    "print lemmatize_tokens_pos(my_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Filtrowanie tokenów nie zawierających słów\n",
    "\n",
    "Poniższa komórka implementuje funkcję, która filtruje z listy tokenów tokeny nie zawierające żadnego znaku alfanumerycznego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'be', u'meeting', u'you', u'tomorrow', u'where', u'do', u'we', u'have', u'our', u'meeting', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "clear_regex = re.compile(ur'\\w')\n",
    "\n",
    "def filter_punctuation(token_list):\n",
    "    filtered_tokens = []\n",
    "    for token in token_list:\n",
    "        if clear_regex.search(token):\n",
    "            filtered_tokens.append(token)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "lemma_tokens = [u'i', u'be', u'meeting', u'you', u'tomorrow', u'.', u'where', u'do', u'we', u'have', u'our', u'meeting', u'?', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u',', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'.', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "print filter_punctuation(lemma_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Filtrowanie słów o małym znaczeniu \n",
    "\n",
    "Filtrowanie słów o małym znaczeniu odbywa się przy wykorzystaniu stop-list (ang. stopwords). Poniższa komórka wczytuje zapisaną na dysku listę z pliku tekstowego oraz implementuje funkcję, która w oparciu o tę listę filtruje słowa o małym znaczeniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'meeting', u'tomorrow', u'meeting', u'sample', u'tweet', u'#hashtag', u'text', u'#anotherhashtag', u'again', u'#yolo', u\"aren't\", u'bad', u'pyprogrammer']\n"
     ]
    }
   ],
   "source": [
    "with open ('common-english-words.txt', 'r') as stopwords_file:\n",
    "    raw_stopwords = stopwords_file.readlines()\n",
    "    \n",
    "stopwords = raw_stopwords[0].split(',')\n",
    "\n",
    "def filter_stopwords(token_list, stopwords_list):\n",
    "    filtered_tokens = []\n",
    "    for token in token_list:\n",
    "        if token not in stopwords_list:\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "lemma_tokens = [u'i', u'be', u'meeting', u'you', u'tomorrow', u'where', u'do', u'we', u'have', u'our', u'meeting', u'some', u'sample', u'tweet', u'with', u'some', u'#hashtag', u'then', u'some', u'text', u'and', u'then', u'#anotherhashtag', u'again', u'#yolo', u'you', u\"aren't\", u'a', u'bad', u'pyprogrammer']\n",
    "print filter_stopwords(lemma_tokens, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Analiza sentymentalna tweetów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5252\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_tweet_sentiment(tweet_sentence):\n",
    "    scores = sentiment_analyzer.polarity_scores(tweet_sentence)\n",
    "    return scores['compound']\n",
    "\n",
    "print calculate_tweet_sentiment(\"What is going on here? We went there yesterday. Where have you gone? I am meeting you tomorrow. Where do we have our meeting? Some sample tweet with some #hashtag, then some text and then #anotherhashtag again #YOLO. You aren't a bad Pyprogrammer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Rozpoznawanie encji (ang. Named entity recognition)\n",
    "Rozpoznawanie encji polega na indentyfikacji w zdaniach tokenów posiadających szczególne znaczenie. Typowe encje, które rozpoznaje się w tym procesie to: Osoba, Miejsce, Organizacja, Czas.\n",
    "Poniższa komórka implementuje metodę, która zwraca listy osób, miejsc i organizacji w danym tweecie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wpisz tutaj swoją funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming danych z Twittera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Ustawienia kluczy i tokenów dla API Twittera\n",
    "\n",
    "W poniższej komórce ustawiane są zmienne niezbędne do uzyskania połączenia z API Twittera. Uzupełnij zmienne o swoje wartości kluczy i tokenów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_token = \"2362404584-MJuLY5ISJq3CFxyDTVhuhI6rRjygCDxd9QYEzWg\"\n",
    "access_token_secret = \"pGD3PyMuz5M6YxzCkAryaytkPD0Eb2lF8q2aI9mNgg07o\"\n",
    "consumer_key = \"uOO1duKiGl0jaRBA9dRvewGXd\"\n",
    "consumer_secret = \"Odsld4Q5fAB9mk9VSJQUPYGDWcepOOUEZZk08Ya9CIR54szd4k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementacja klasy służącej do Streamingu danych z Twittera\n",
    "\n",
    "W poniższej komórce implementowana jest klasa służąca do pobierania streamu danych z Twittera. Klasa ta dziedziczy klasę StreamListener pochodzącą z biblioteki tweepy (biblioteki służącej do łączenia się z API Twittera za pomocą Pythona).\n",
    "Implementacja poniższej klasy modyfikuje domyślną metodę on_status(), która uruchamiana jest przy pojawieniu się każdego nowego statusu (tweeta) na Twitterze. \n",
    "\n",
    "Funkcja on_status() zapisuje każdego tweeta do bazy danych Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200/\")\n",
    "\n",
    "\n",
    "class StreamProcessingListener(StreamListener):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        created_at = status.created_at\n",
    "        text = status.text\n",
    "        user_description = status.user.description\n",
    "        user_location = status.user.location\n",
    "        coords = status.coordinates\n",
    "        user_name = status.user.screen_name\n",
    "        user_created = status.user.created_at\n",
    "        followers = status.user.followers_count\n",
    "        id_str = status.id_str\n",
    "        retweets = status.retweet_count\n",
    "        bg_color = status.user.profile_background_color\n",
    "        \n",
    "        processed_text = sample_processing(text)\n",
    "        hashtags_list = get_hashtags_list(processed_text)\n",
    "        tokens = tokenize_tweets(processed_text)\n",
    "        stems = stem_tokens(tokens)\n",
    "        lemmas = lemmatize_tokens_pos(tokens)\n",
    "        filtered_tokens = filter_punctuation(tokens)\n",
    "        filtered_tokens = filter_stopwords(filtered_tokens, stopwords)\n",
    "        sentiment = calculate_tweet_sentiment(processed_text)        \n",
    "        \n",
    "        es.index(index=\"twitter\",\n",
    "             doc_type=\"tweet\",\n",
    "             body={\n",
    "                \"created_at\": created_at,\n",
    "                \"text\": text,\n",
    "                \"user_description\": user_description,\n",
    "                \"user_location\": user_location,\n",
    "                \"coords\": coords,\n",
    "                \"user_name\": user_name,\n",
    "                \"user_created\": user_created,\n",
    "                \"followers\": followers,\n",
    "                \"id_str\": id_str,\n",
    "                \"retweets\": retweets,\n",
    "                \"bg_color\": bg_color,\n",
    "                \"processed_text\": processed_text,\n",
    "                \"hashtags_list\": hashtags_list,\n",
    "#                 \"stems\": stems,\n",
    "#                 \"lemmas\": lemmas,\n",
    "                \"tokens\": filtered_tokens,\n",
    "                \"sentiment\": sentiment})\n",
    "\n",
    "        \n",
    "        print \"text\" + text\n",
    "        print sentiment\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Nawiązanie połączenia z API Twittera i uruchomienie stremingu\n",
    "\n",
    "W poniższej komórce nawiązywane jest połączenie z Twitterem za pomocą danych uwierzytelniających użytkownika a następnie uruchamiany jest 20 sekundowy streaming danych z przykładowym filtrem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textRT @EinzelfallXy: @HartesGeld Screenshot von @HartesGeld \n",
      "zu #LePen #Macronleaks https://t.co/plLf2a8P74\n",
      "0.0\n",
      "textRT @Purple_POV: Make France Great Again. 🇫🇷 @MLP_officiel #Bayrougate #lepen #presidentielle2017  #jevotepour  #ChoisirLaFrance… \n",
      "0.6249\n",
      "textRT @AndreaChalupa: Please, please, please may the French media be more sophisticated and intelligent than the American media. Please. #EMLe…\n",
      "0.9366\n",
      "text@dodt2003 @marioadragna69 @GeorgiaDirtRoad @Root4Change @gearmeister @InTheYear1611 @MsRotti @tamaraleighllc… https://t.co/LE8D2rYC7u\n",
      "0.0\n",
      "textRT @GayRepublicSwag: Who would you rather see as the next President of #France retweet for a large sample size. #LePen #Macron #MacronGate\n",
      "0.0\n",
      "textRT @vicktop55: French voters are most polarized in Europe ahead of key presidential runoff – poll https://t.co/g2enxxnvTs\n",
      "0.0\n",
      "textRT @ProudWednesdayy: REWTEET if you stand with Marine LePen.  THIS is what a STRONG WOMAN looks like #saturdaymorning #jevotepour… \n",
      "0.7603\n",
      "textRT @CodeAud: #ISIS calls for French election day attacks &amp; urges slaughter of #LePen &amp; #Macron https://t.co/UsZXP7ZVDu https://t.co/FKBcCCb…\n",
      "-0.4404\n",
      "text@TerrySechelski I agree but I still have hope that LePen can turn it around ....KeK wills it!\n",
      "0.7088\n",
      "textRT @sto93_NotS: @mom_vet @Kobayautiste @Shamira_1925 @nia4_trump @Dutch_Deplorabl @SiriusMatdj @VictorOfKadesh @KansasCity1980… \n",
      "0.0\n",
      "textRT @MattAsherS: We are tweeting in support of #MacronPresident from 3-4ET b/c #altright &amp; #Russia 💗 #LePen.\n",
      "\n",
      "Join us &amp; use➡️… \n",
      "0.5994\n",
      "textRT @Pamela_Moore13: #MacronLeaks\n",
      "\n",
      "Dems: Trump shouldn't support #LePen. It's interfering w/ the French election\n",
      "Obama: \"I support… \n",
      "-0.3089\n",
      "textRT @wmmII88: #MSM in full panic as #MacronLeaks spreads to primetime TV. Keep up the pressure. We are the media now. #LePen https://t.co/82…\n",
      "-0.6705\n",
      "textRT @Roses_4_Thorns: #MSM in full panic as #MacronLeaks spreads to primetime TV. Keep up the pressure. We are the media now. #LePen https://…\n",
      "-0.6705\n",
      "textRT @MattAsherS: The American resistance supports #macronprésident in #ElectionsPresidentielles2017🇫🇷 because voting for #LePen is voting fo…\n",
      "0.3612\n",
      "textRT @IMPL0RABLE: #TheResistance #MacronLeaks #Macron #LePen #AVote\n",
      "\n",
      "Interesting thread: https://t.co/RD4cYP60bB\n",
      "0.4019\n",
      "textRT @sto93_NotS: @mom_vet @Kobayautiste @Shamira_1925 @nia4_trump @Dutch_Deplorabl @SiriusMatdj @VictorOfKadesh @KansasCity1980… \n",
      "0.0\n",
      "textRT @GrrrGraphics: #MacronLeaks #MacronGate #frenchelection #BenGarrison #cartoon Welcome Mat for #Terror #JeVote #LePen #JeVoteMarine… \n",
      "0.4588\n",
      "textvoters hurling eggs and flour??? https://t.co/whBxWsX6hF\n",
      "0.0\n",
      "text@klaasvaak1533 Wilders lost and so will LePen! https://t.co/L5f2q0IsAr\n",
      "-0.3802\n",
      "textRT @LindaSuhler: French Presidential Voting Underway as Overseas Electorate Head to Polls\n",
      "#VoteMarine #LePen #Frexit… \n",
      "0.0\n",
      "textRT @Mags661: 💥🚨 THE RUSSIANS!!!!!🚨🔥\n",
      "#MacronLeaks\n",
      "\n",
      "#LePen 🇫🇷 https://t.co/JetrJkA7Bd\n",
      "0.0\n",
      "textRT @Purple_POV: Make France Great Again. 🇫🇷 @MLP_officiel #Bayrougate #lepen #presidentielle2017  #jevotepour  #ChoisirLaFrance… \n",
      "0.6249\n",
      "textRT @AFP: Paradise lost: idyllic French villages turn to far right - @ADAMPLOW reports https://t.co/ZjBP3tztDY… \n",
      "0.4404\n",
      "textRT @womensmarchlon: France. On Sunday show the world you stand for inclusion and equality. Say no to LePen #VoteAgainstHate #chooselove htt…\n",
      "-0.296\n",
      "textRT @starbuck2017: We're supporting #JeSoutiensMacron from 3-4ET Sat #Altright &amp; #Russia 💗#LePen\n",
      "Join us &amp; use #Presidentielle2017  --… \n",
      "0.6249\n",
      "textRT @ChooseToBFree: 🇫🇷MAKE FRANCE GREAT AGAIN🇫🇷\n",
      "#MarineLePen✅\n",
      "#France\n",
      "#2017LeDébat  \n",
      "#Debat2017 \n",
      "#JeVote \n",
      "#LePen\n",
      "#Presidentielle2017 https:/…\n",
      "0.7034\n",
      "textRT @Mags661: 💥🚨 THE RUSSIANS!!!!!🚨🔥\n",
      "#MacronLeaks\n",
      "\n",
      "#LePen 🇫🇷 https://t.co/JetrJkA7Bd\n",
      "0.0\n",
      "textRT @North_Going_Zax: Vote smart vote to stop the British LePen @theresa_may &amp; her far right coalition of chaos https://t.co/hYMGyyb3Tt\n",
      "-0.4939\n",
      "textRT @GrrrGraphics: #MacronLeaks #MacronGate #frenchelection #BenGarrison #cartoon Welcome Mat for #Terror #JeVote #LePen #JeVoteMarine… \n",
      "0.4588\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "listener = StreamProcessingListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "stream = Stream(auth, listener)\n",
    "\n",
    "#This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "stream.filter(track=['LePen'], languages=['en'], async=True)\n",
    "time.sleep(60)\n",
    "stream.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
